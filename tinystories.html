<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="styles.css">
  <script src="https://kit.fontawesome.com/92a7126d32.js" crossorigin="anonymous"></script>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Montserrat+Alternates:ital,wght@1,500&family=Montserrat:wght@400;500&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Abril+Fatface&display=swap" rel="stylesheet">
  <title>Coumba's Portfolio</title>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.4.0/p5.js"></script>
  <script src="myanimations.js"></script>
  <script src="app.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" 
    integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" 
    crossorigin="anonymous"></script>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" 
    rel="stylesheet" 
    integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" 
    crossorigin="anonymous">
</head>
<body>
  <div class="header">
    <a class="close" href="/"><i class="fa-solid fa-x" style="color: #ffffff;"></i> Close Project</a>
    <div class="header-content">
      <h2>Building a Small Language Model</h2>
      <img src="assets/placeholder.png" alt="Project Image" style="max-width:100%; border-radius:10px; margin-top:1rem;">
      <p>Large language models like GPT-3 and GPT-4 can write fluent and coherent text, but they come at a massive computational cost. 
      This inspired the question behind the paper <em>TinyStories: How Small Can Language Models Be and Still Speak Coherent English?</em>: 
      <strong>can we shrink language models to under 10M parameters and still produce human-like stories?</strong></p>
      <p>Together with <strong>Nuremir Babanov</strong>, I set out to replicate the TinyStories paper by training a GPT-2–style model with only 
      <strong>8.89M parameters</strong> on a dataset of children’s stories. Our goal was to test the boundaries of small language models while 
      learning about optimization, architecture, and evaluation. This was presented as a final project for our course CS 449: Deep Learning.</p>
      <ul>
        <li><span>Collaboration</span><p> With Nuremir Babanov</p></li>
        <li><span>Period</span><p>Spring 2024</p></li>
      </ul>
      <ul>
        <li><span>Tools</span><p> PyTorch, HuggingFace</p></li>
      </ul>
    </div>
  </div>

  <div class="project-container">
    <h3>Dataset: TinyStories</h3>
    <p>We trained on <strong>4.97M synthetic short stories</strong>, generated by GPT-3.5/4, specifically designed for the vocabulary of 
    3–4 year-olds. Each story contains 2–3 paragraphs with simple plots and consistent themes. Using SentencePiece, we created a 
    custom tokenizer with a <strong>10K vocabulary</strong>, allowing the model to capture child-level English effectively.</p>

    <h3>Baselines</h3>
    <p>To measure success, we compared our model against two baselines:</p>
    <ul>
      <li><strong>N-Gram (Trigram)</strong> → produced incoherent, repetitive text.</li>
      <li><strong>GPT-2 (125M)</strong> → served as a higher-capacity benchmark.</li>
    </ul>

    <h3>Model Architecture</h3>
    <p>We implemented a GPT-2–style model with the following configuration:</p>

    <ul>
      <li>8 transformer layers, 8 attention heads each</li>
      <li>Embedding size: 256 | Sequence length: 512</li>
      <li>Total parameters: <strong>8.89M</strong></li>
      <li>Optimizer: <strong>AdamW + warm-up + cosine annealing</strong></li>
      <li>Training time: <strong>6 hours on a single RTX 4090</strong> (mixed precision)</li>
    </ul>
    

    <h3>Results</h3>
    <p>Our small model achieved results comparable to the TinyStories paper:</p>
    <ul>
      <li><strong>Final Loss:</strong> 1.25 (vs. 1.3 in paper)</li>
      <li><strong>Perplexity:</strong> 3.49 (matches paper)</li>
    </ul>
    <p>Using GPT-4 to evaluate 50 story completions:</p>
    <ul>
      <li><strong>SLM (ours):</strong> Grammar 6.9 / Creativity 6.8 / Consistency 5.9</li>
      <li><strong>GPT-2 (125M):</strong> Grammar 5.2 / Creativity 7.2 / Consistency 6.6</li>
      <li><strong>N-Gram:</strong> Grammar 3.8 / Creativity 3.1 / Consistency 1.6</li>
    </ul>

    <h3>Key Insights</h3>
    <p>Our work shows that with the right dataset and training setup, 
    <strong>a model with fewer than 9M parameters can generate coherent, child-level stories</strong>. 
    While it cannot scale to more complex reasoning tasks, this experiment highlights the potential of small language models as 
    lightweight, resource-efficient alternatives for domain-specific applications.</p>

    <h3>Resources</h3>
    <p>Check out the trained model and checkpoints on <a href="https://huggingface.co/AyNio/tinystories-GPT-2-9M/tree/main" target="_blank">HuggingFace</a>.</p>
    <p> Read our final presentation <a href="https://docs.google.com/presentation/d/e/2PACX-1vRQ0U-1Q7t62w_pnhhMFeABrY2HewqciTBmZm25gP5s4DH4qrGfZ7NcoELtGKhsP1D_vF9IBvJEvdc2/pub?start=false&loop=false&delayms=3000https://docs.google.com/presentation/d/e/2PACX-1vRQ0U-1Q7t62w_pnhhMFeABrY2HewqciTBmZm25gP5s4DH4qrGfZ7NcoELtGKhsP1D_vF9IBvJEvdc2/pub?start=false&loop=false&delayms=3000">here.</a></p>
    <p>Find the original paper that inspired this work <a href="https://arxiv.org/abs/2305.07759"> here.</a></p>
</div>

  <footer></footer>
</body>
</html>
<!DOCTYPE html>